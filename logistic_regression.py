# -*- coding: utf-8 -*-
"""logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KQf4L1so9I7NhPQfoUrxgyqQISRm1TI

Logistic Regression Block
"""

import os
import gensim
import string
import nltk
import itertools
import json
import gzip
import pandas as pd
import numpy as np

from nltk.stem.porter import PorterStemmer
from string import digits
from statistics import mean 
from nltk.corpus import stopwords
from collections import Counter
from prettytable import PrettyTable


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn import model_selection, naive_bayes, svm

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


def get_tfidf(df, min_df, ngram_range, penalty):
    '''Get tfidf vectors for the cleaned labels in the dataframe.'''
    
    tfidf = TfidfVectorizer(sublinear_tf = True, 
                            min_df = min_df, 
                            max_features = min_df, # set these equal to one another
                            norm = penalty, 
                            encoding = 'latin-1', 
                            max_df = 0.3,
                            binary = False,
                            ngram_range = ngram_range, 
                            stop_words = 'english')

    features = tfidf.fit_transform(df.reviews_cleaned.tolist())
    labels = df.label
    print("Number of features:", features.shape[1]) 
    
    return features, labels, tfidf

def T_T_split(features, labels):
    X_train, X_test, y_train, y_test = train_test_split(
                                        features, 
                                        labels, 
                                        test_size=0.33 #random_state = 43
                                        )
    
    return X_train, X_test, y_train, y_test

def fit_logistic(df, model_name, min_df, ngram_range, penalty):

    print("Running", model_name, "with parameters ngram_range = ", ngram_range, "and min_df = ", min_df)
    print(" ")

    # (optional) shorten dataframe to speed up modeling
    if df.shape[0] > 200000:
      df = df[:200000]
  
    features, labels, tfidf = get_tfidf(df, min_df, ngram_range, penalty)

    X_train, X_test, y_train, y_test = T_T_split(features, labels)
    
    model = LogisticRegression(random_state = 0,
                               class_weight = 'balanced',
                               penalty = penalty)
    
    model.fit(X_train, y_train)

    CV = 5
    cv_df = pd.DataFrame(index = range(CV))

    accuracy = cross_val_score(model, 
                               X_train, 
                               y_train, 
                               scoring = 'accuracy', 
                               cv = CV)

    roc = cross_val_score(model, 
                               X_train, 
                               y_train, 
                               scoring = 'roc_auc', 
                               cv = CV)
    
    idf = tfidf.idf_
    feat_importances = dict(zip(tfidf.get_feature_names(), idf))
    d = Counter(feat_importances)

    y_pred = model.predict(X_train)
    print("Average accuracy is", round(mean(accuracy), 3))
    print("Average ROC_AUC is", round(mean(roc), 3))
    print('Confusion matrix:')
    print(confusion_matrix(y_train, y_pred)); print(" ")
    
    # print("Top 10 most important features:")
    # for k, v in d.most_common(10):
    #     print('%s: %i' % (k, round(v, 5)))

    print(" ")
    
    return model, round(mean(accuracy), 3), round(mean(roc), 3)

def logistic_runner():
    df = pd.read_csv("home_and_kitchen_ready_to_model.csv")
    df = df[['label', 'reviews_cleaned']]
    df.dropna(inplace=True)
    print('Pulled dataframe from file.'); print(" ")

    model_names = ['Logistic1', 'Logistic2', 'Logistic3', 'Logistic4', 'Logistic5', 'Logistic6', 'Logistic7', 'Logistic8']
    ngram_ranges = [(1,1), (1,2), (1,1), (1,2), (1,1), (1,2), (1,1), (1,2)] 
    penalty_values = ['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2']
    min_df_values = [500, 500, 500, 500, 200, 200, 200, 200]
    
    tab = PrettyTable()
    tab.field_names = ["Model Name", "Min_Df", "Ngram_Range", "Penalty", "Accuracy", "ROC"]

    num_models = 8

    for i in range(0, num_models-1):
        model, acc, roc = fit_logistic(df, 
                                       model_name = model_names[i],
                                       min_df = min_df_values[i],
                                       ngram_range = ngram_ranges[i],
                                       penalty = penalty_values[i])
        
        tab.add_row([model_names[i], min_df_values[i], ngram_ranges[i], penalty_values[i], acc, roc])
    
    print(tab)


if __name__ == "__main__":
    
    logistic_runner()

'''
#####_____Results_____#####

For this experiment I varied the number of ngrams (1 or 2), the penalty or norm (L1 or L2) used in regularization, and the Min_DF for TF-IDF, which controls the number of features created. I used Accuracy and ROC score for my reporting metrics - Accuracy because I want to know how the model performs as far as the misclassification rate, and ROC because it is a measure of how well the model performs on an unbalanced dataset, and this dataset was fairly unbalanced with more positive (3+ star) reviews than negative (1-2 star) reviews.

I took the following preprocessing steps:
*   Labelling reviews with 3+ stars "positive" (1) and reviews with 1 or 2 stars "negative" (0).
*   Removing all numbers from the text
*   Removing all punctuation from the text
*   Removing all stopwords (English) from the text
*   Stemming all words

I used the following parameters identically for all models:

*   Classes balanced. I did not want the model to predict based on the probability of a positive review, so I set class_weights to balanced in the Logistic Regression parameters.
*   Latin encoding. All 
*   Max_DF of 0.3 (threshold for ignoring corpus-specific stop words) - I set this value fairly low because I did not want to ignore too many words. Since this is a very varied dataset, there will be many words that are important to context and also are repeated many times.
*   Binary = False (items are counted in the number that they appear). Since there will be some important words that are used very few times, I didn't want them to be overshadowed (or considered equal to) words that are less important but used many times.

All models performed decently well, with accuracies ranging from 0.73 to 0.81 and ROC values ranging from 0.83 to 0.89. The best-performing model on both metrics used L2 regularization, a feature size of 500, and 2 ngrams. 
This is likely because the model was given more information to work with (both with bigrams and more features) and L2 regularization was used to cut the model to include only useful features. 
Overall, the model could likely be improved upon if I used more features, more n-grams, and a lower max_df, but all of these things would increase training time too much to be scalable.'''
