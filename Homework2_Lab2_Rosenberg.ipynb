{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to repo & branch: https://github.com/rrosenb1/rrosenb1_msia490_2019/tree/homework2\n",
    "\n",
    "Remember to:\n",
    "- Submit .py file\n",
    "- If there are multiple files in the repo, include a README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import digits\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7522 files parsed.\n",
      "Returning first 7000 files.\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "# Working with full dataset from last lab assignment\n",
    "def retrieve_data(end_len):\n",
    "    path = '/Users/rachelrosenberg/MSiA/490 - Text Analytics/20news-18828/'\n",
    "    textdata = []\n",
    "    files_parsed = 0\n",
    "\n",
    "    for folder in os.listdir(path):\n",
    "        try:\n",
    "            for filename in os.listdir(str(path + folder)):\n",
    "                with open(str(path + folder + '/' + filename), 'r') as f:\n",
    "                    text = f.read()\n",
    "                    textdata.append(text)\n",
    "                    files_parsed += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(files_parsed, \"files parsed.\")\n",
    "    print(\"Returning first\", end_len, \"files.\")\n",
    "    \n",
    "    return(textdata[:end_len])\n",
    "\n",
    "textdata_raw = retrieve_data(7000)\n",
    "print(len(textdata_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation. New length = 7000\n",
      "Converted to lowercase. New length = 7000\n",
      "Removed stopwords. New length = 7000\n",
      "Removed numbers. New length = 7000\n",
      "Stemmed words. New length = 7000\n",
      "[['hmcsbrownedu', 'harri', 'mamayski', 'subject', 'heil', 'hernlem', 'articl', 'aprncsuedu', 'hernlemchessncsuedu', 'brad', 'hernlem', 'write', 'lebanes', 'resist', 'forc', 'deton', 'bomb', 'isra', 'occup', 'patrol', 'lebanes', 'territori', 'two', 'day', 'ago', 'three', 'soldier', 'kill', 'two', 'wound', 'retali', 'isra', 'israeliback', 'forc', 'wound', '', 'civilian', 'bombard', 'sever', 'lebanes', 'villag', 'iron', 'isra', 'govern', 'justifi', 'occup', 'lebanon', 'claim', 'necessari', 'prevent', 'bombard', 'isra', 'villag', 'congratul', 'brave', 'men', 'lebanes', 'resist', 'everi', 'isra', 'son', 'place', 'grave', 'underlin', 'moral', 'bankruptci', 'israel', 'occup', 'draw', 'attent', 'isra', 'govern', 'polici', 'reckless', 'disregard', 'civilian', 'life', 'brad', 'hernlem', 'hernlemchessncsuedu', 'nice', 'three', 'peopl', 'murder', 'bradli', 'overjoy', 'hear', 'death', 'middl', 'east', 'jewish', 'arab', 'death', 'feel', 'sad', 'hope', 'soon', 'stop', 'appar', 'view', 'point', 'accept', 'peopl', 'like', 'bradli', 'hernlem', 'disgust', 'harri'], ['waldocybernetcsefauedu', 'todd', 'j', 'dicker', 'subject', 'israel', 'expans', 'ii', 'abzvirginiaedu', 'andi', 'beyer', 'write', '', 'first', 'never', 'said', 'holocaust', 'said', '', 'holocaust', 'im', 'ignor', 'holocaust', 'know', '', 'nazi', 'germani', 'peopl', 'mayb', 'includ', 'uh', 'oh', 'first', 'sign', 'argument', 'without', 'meritth', 'state', 'one', 'qualif', 'area', 'know', 'someth', 'nazi', 'germani', 'show', 'dont', 'shut', 'simpl', '', 'dont', 'think', 'suffer', 'jew', 'wwii', '', 'justifi', 'crime', 'commit', 'isra', 'govern', '', 'attempt', 'call', 'civil', 'liberterian', 'like', 'antisemet', '', 'appreci', 'jew', 'suffer', 'wwii', 'belov', 'perish', 'tortur', 'suffer', 'second', 'namecal', 'direct', 'civillibertarian', 'gener', 'namedrop', 'fanci', 'sound', 'polit', 'term', 'yet', 'anoth', 'attempt', 'cite', 'qualif', 'order', 'obfusc', 'glare', 'unprepared', 'argument', 'go', 'back', 'minor', 'junior']]\n"
     ]
    }
   ],
   "source": [
    "# Clean & normalize dataset\n",
    "# normalization (e.g. convert to lowercase, remove non-alphanumeric chars, numbers,\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "textdata = textdata_raw\n",
    "\n",
    "def strip(textdata):\n",
    "    textdata_stripped = []\n",
    "\n",
    "    for l in textdata:\n",
    "        # split into words by white space\n",
    "        words = l.split()\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        textdata_stripped.append([w.translate(table) for w in words])\n",
    "    \n",
    "    return textdata_stripped\n",
    "\n",
    "def to_lower(textdata):\n",
    "    textdata_lower = []\n",
    "    \n",
    "    for l in textdata:\n",
    "        # convert to lower case\n",
    "        textdata_lower.append([word.lower() for word in l])\n",
    "    \n",
    "    return textdata_lower\n",
    "\n",
    "def rm_stopwords(textdata):\n",
    "    # filter out stop words\n",
    "    words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for l in textdata:\n",
    "        words.append([word for word in l if not word in stop_words])\n",
    "    \n",
    "    return words\n",
    "\n",
    "def rm_numbers(textdata):\n",
    "    words = []\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    \n",
    "    for l in textdata:\n",
    "        words.append([word.translate(remove_digits) for word in l])\n",
    "    \n",
    "    return words\n",
    "    \n",
    "def stem_words(textdata):\n",
    "    words = []\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    for l in textdata:\n",
    "        words.append([porter.stem(word) for word in l])\n",
    "\n",
    "    return words\n",
    "\n",
    "textdata = strip(textdata)\n",
    "print(\"Removed punctuation. New length =\", len(textdata))\n",
    "textdata = to_lower(textdata)\n",
    "print(\"Converted to lowercase. New length =\", len(textdata))\n",
    "textdata = rm_stopwords(textdata)\n",
    "print(\"Removed stopwords. New length =\", len(textdata))\n",
    "textdata = rm_numbers(textdata)\n",
    "print(\"Removed numbers. New length =\", len(textdata))\n",
    "textdata = stem_words(textdata)\n",
    "print(\"Stemmed words. New length =\", len(textdata))\n",
    "\n",
    "print(textdata[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output as a text file with one document per line\n",
    "f = open(\"lab2results_Rosenberg.txt\",\"w+\")\n",
    "\n",
    "for doc in textdata[0:10]:\n",
    "    f.write(' '.join(doc))\n",
    "    f.write('\\n\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmcsbrownedu harri mamayski subject heil hernlem articl aprncsuedu hernlemchessncsuedu brad hernlem write lebanes resist forc deton bomb isra occup patrol lebanes territori two day ago three soldier kill two wound retali isra israeliback forc wound  civilian bombard sever lebanes villag iron isra govern justifi occup lebanon claim necessari prevent bombard isra villag congratul brave men lebanes resist everi isra son place grave underlin moral bankruptci israel occup draw attent isra govern polici reckless disregard civilian life brad hernlem hernlemchessncsuedu nice three peopl murder bradli overjoy hear death middl east jewish arab death feel sad hope soon stop appar view point accept peopl like bradli hernlem disgust harri\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview results doc\n",
    "f = open(\"lab2results_Rosenberg.txt\",\"r+\")\n",
    "output = []\n",
    "\n",
    "for line in f:\n",
    "    output.append(line)\n",
    "    \n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 Part 1\n",
    "1. After completing the lab assignment, experiment with 2 or more sets of word2vec model parameters, for example, different embedding sizes, CBOW vs. skip-gram models, etc. Provide a paragraph or two of qualitative evaluation of your embeddings (no need to provide a quantitative evaluation). For example, you can evaluate the embeddings by hand-picking ~10 words and manually reviewing/evaluating their closest neighbors in terms of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model uses skip-gram and softmax tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(\n",
    "        textdata,\n",
    "        size = 200,\n",
    "        sg = 1, # use skip-gram\n",
    "        hs = 1, # use hierarchical softmax\n",
    "        iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('societi', 0.44188982248306274),\n",
       " ('crux', 0.40291404724121094),\n",
       " ('klux', 0.38458943367004395),\n",
       " ('agenda', 0.38446104526519775),\n",
       " ('ideolog', 0.3736516833305359),\n",
       " ('parti', 0.3689371347427368),\n",
       " ('individu', 0.3670465350151062),\n",
       " ('keithccocaltechedu', 0.36505645513534546),\n",
       " ('invol', 0.358116090297699),\n",
       " ('hypocrisi', 0.35464829206466675)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'polit' # stemmed version of 'politics'\n",
    "model1.wv.most_similar(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hundr', 0.6938508749008179),\n",
       " ('riflemen', 0.4558294117450714),\n",
       " ('twenti', 0.4538172781467438),\n",
       " ('thirti', 0.4189767837524414),\n",
       " ('year', 0.4153338074684143),\n",
       " ('million', 0.4071618616580963),\n",
       " ('bitli', 0.4050976037979126),\n",
       " ('refuge', 0.3979335427284241),\n",
       " ('erzincan', 0.392652690410614),\n",
       " ('acr', 0.3860400915145874)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = 'thousand'\n",
    "model1.wv.most_similar(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('land', 0.4887744188308716),\n",
       " ('arab', 0.4550645053386688),\n",
       " ('peopl', 0.44194215536117554),\n",
       " ('wealthi', 0.42973756790161133),\n",
       " ('thet', 0.4263495206832886),\n",
       " ('world', 0.4240114092826843),\n",
       " ('us', 0.4212038516998291),\n",
       " ('european', 0.41993874311447144),\n",
       " ('expansion', 0.41251832246780396),\n",
       " ('live', 0.40439772605895996)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = 'countri'\n",
    "model1.wv.most_similar(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model uses CBOW tuning and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(\n",
    "        textdata,\n",
    "        size = 200,\n",
    "        sg = 0, # use CBOW\n",
    "        hs = 0, # use negative sampling\n",
    "        iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ideolog', 0.479442298412323),\n",
       " ('agenda', 0.4705851078033447),\n",
       " ('econom', 0.4452664852142334),\n",
       " ('racial', 0.44172555208206177),\n",
       " ('stake', 0.41370296478271484),\n",
       " ('sole', 0.4134103059768677),\n",
       " ('divers', 0.40592777729034424),\n",
       " ('philosophi', 0.3986875116825104),\n",
       " ('prolif', 0.39447957277297974),\n",
       " ('coalit', 0.3930385112762451)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'polit' # stemmed version of 'politics'\n",
    "model2.wv.most_similar(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hundr', 0.7453307509422302),\n",
       " ('twenti', 0.6143282651901245),\n",
       " ('fifti', 0.597360372543335),\n",
       " ('dozen', 0.57829749584198),\n",
       " ('million', 0.5748527646064758),\n",
       " ('approxim', 0.5406707525253296),\n",
       " ('hunder', 0.5385438203811646),\n",
       " ('refuge', 0.5160118341445923),\n",
       " ('half', 0.5053600072860718),\n",
       " ('deport', 0.5041437149047852)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = 'thousand'\n",
    "model2.wv.most_similar(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('economi', 0.4222654700279236),\n",
       " ('vast', 0.4147487282752991),\n",
       " ('democraci', 0.40389731526374817),\n",
       " ('britain', 0.4017581343650818),\n",
       " ('germani', 0.40147364139556885),\n",
       " ('succeed', 0.39928996562957764),\n",
       " ('citizenship', 0.3979325592517853),\n",
       " ('wealthi', 0.397931843996048),\n",
       " ('treat', 0.3917727470397949),\n",
       " ('european', 0.3860580325126648)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = 'countri'\n",
    "model2.wv.most_similar(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model uses skip-gram and negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = gensim.models.Word2Vec(\n",
    "        textdata,\n",
    "        size = 200,\n",
    "        sg = 1, # use skip-gram\n",
    "        hs = 0, # use negative sampling\n",
    "        iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crux', 0.46794018149375916),\n",
       " ('uci', 0.44179970026016235),\n",
       " ('covert', 0.42880165576934814),\n",
       " ('academia', 0.4243655204772949),\n",
       " ('impot', 0.40385329723358154),\n",
       " ('invol', 0.40143439173698425),\n",
       " ('reshap', 0.40044814348220825),\n",
       " ('monarch', 0.3960481584072113),\n",
       " ('clout', 0.3960346579551697),\n",
       " ('irrespect', 0.393835186958313)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'polit' # stemmed version of 'politics'\n",
    "model3.wv.most_similar(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hundr', 0.6340253353118896),\n",
       " ('riflemen', 0.5023699998855591),\n",
       " ('hunder', 0.4799419641494751),\n",
       " ('trabzon', 0.45165306329727173),\n",
       " ('rehir', 0.45126426219940186),\n",
       " ('livelihood', 0.4443967938423157),\n",
       " ('glyph', 0.4428291916847229),\n",
       " ('twenti', 0.4341743588447571),\n",
       " ('acr', 0.43135836720466614),\n",
       " ('deport', 0.42930537462234497)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = 'thousand'\n",
    "model3.wv.most_similar(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('america', 0.4656364917755127),\n",
       " ('armsnevernevernev', 0.4558003544807434),\n",
       " ('acr', 0.4442521929740906),\n",
       " ('telaviv', 0.4336003065109253),\n",
       " ('ecconomi', 0.42291903495788574),\n",
       " ('englishman', 0.41559451818466187),\n",
       " ('expansion', 0.41443145275115967),\n",
       " ('arabian', 0.4144164025783539),\n",
       " ('calam', 0.4112203121185303),\n",
       " ('hungari', 0.4111582040786743)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = 'countri'\n",
    "model3.wv.most_similar(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth model uses negative sampling and CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = gensim.models.Word2Vec(\n",
    "        textdata,\n",
    "        size = 200,\n",
    "        sg = 0, # use CBOW\n",
    "        hs = 1, # use hierarchical softmax\n",
    "        iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ideolog', 0.2981662154197693),\n",
       " ('econom', 0.2893720269203186),\n",
       " ('usca', 0.28250837326049805),\n",
       " ('involv', 0.27779242396354675),\n",
       " ('forc', 0.2678338885307312),\n",
       " ('reinvent', 0.2653023898601532),\n",
       " ('posit', 0.262771338224411),\n",
       " ('leftist', 0.26245975494384766),\n",
       " ('desir', 0.26115232706069946),\n",
       " ('implic', 0.2609318792819977)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'polit' # stemmed version of 'politics'\n",
    "model4.wv.most_similar(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('million', 0.3267165422439575),\n",
       " ('flee', 0.31789684295654297),\n",
       " ('earthli', 0.3094852566719055),\n",
       " ('twenti', 0.3011215329170227),\n",
       " ('compund', 0.2995664179325104),\n",
       " ('whereabout', 0.2929856479167938),\n",
       " ('hundr', 0.27984845638275146),\n",
       " ('hungarian', 0.27687346935272217),\n",
       " ('approxim', 0.274114191532135),\n",
       " ('refuge', 0.2739272117614746)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = 'thousand'\n",
    "model4.wv.most_similar(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.3884241282939911),\n",
       " ('peopl', 0.3606693148612976),\n",
       " ('us', 0.3505263924598694),\n",
       " ('jew', 0.33863359689712524),\n",
       " ('russia', 0.31717199087142944),\n",
       " ('economi', 0.3112293481826782),\n",
       " ('nonjew', 0.29754510521888733),\n",
       " ('wealthi', 0.29508641362190247),\n",
       " ('societi', 0.2901819944381714),\n",
       " ('planet', 0.28953540325164795)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = 'countri'\n",
    "model4.wv.most_similar(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Model 1: Skip-Gram and Hierarchical Softmax tuning\n",
    "\"polit\" - does not do well. Captures similar words like \"ideology\" and \"society\" but its cosine similarity values are smaller than they should be.\n",
    "\n",
    "\"thousand\" - model does much better here. Captures similar number-related words like \"hundred\", \"twenty\", and \"million\", and words that go with these numbers in military context like \"riflemen\" (as in, \"the army had a thousand riflemen\"). However, it also picks some nonsensical words, like \"erzincan\".\n",
    "\n",
    "\"countri\" (country) - the model captures words that make sense, like \"land\", \"european\", and \"world\". However, its confidences of similarities are still very low.\n",
    "\n",
    "Overall: Overall, this is not a great model. The combination of skip-gram and softmax is not ideal for this data.\n",
    "\n",
    "#### Model 2: CBOW and Negative Sampling tuning\n",
    "\"polit\" - This model does somewhat better than the first, but the highest cosine similarity value is still a low 0.48. However, it does catch words like \"economy\", \"ideology\", and \"agenda\".\n",
    "\n",
    "\"thousand\" - This model does much better than the first, capturing similar word \"hundred\" with a similarity value of 0.75. This is the most sensical vector so far, with words like \"twenty\", \"dozen\", and \"fifty\" being captured with high similarity values.\n",
    "\n",
    "\"countri\" - This model does slightly worse than the first model, capturing similar words but with a max similarity value of 0.42. However, words like \"european\", \"germany\", and \"britain\" are in fact similar and are captured by this model.\n",
    "\n",
    "Overall: Overall, this model performs about the same as the first model. \"polit\" and \"countri\" are consistently harder to train, with \"thousand\" being consistently the word with the best similarity vectors. Parameter-wise, this is the opposite of the first model; this may indicate that the dataset is just not large enough or is too sparse for most of these words to have high levels of context around them.\n",
    "\n",
    "#### Model 3: Skip-Gram and Negative Sampling tuning\n",
    "\"polit\" - This is not a great result, with the word \"uci\" as the \"most similar\" with a similarity of only 0.43 (and being nonsensical). It does capture some other similar words, like \"ideology\" and \"chancellor\", but generally does a poor job.\n",
    "\n",
    "\"thousand\" - The first several words in this similarity vector (\"hundred\", \"riflemen\") make sense, but it breaks down towards the end. Overall this is not a strong result, despite cosine similarity values being high.\n",
    "\n",
    "\"countri\" - This is the strongest result for Model 3, with strong words like \"telaviv\" and \"englishmen\" and one funny entry of \"armsnevernevernev\" (which really should have been removed in tf-idf). \n",
    "\n",
    "Overall: The combination of skip-gram and negative sampling is not a great one. It appears that skip-gram may be the problem; CBOW performs better in these experiments. The next experiment will determine whether hierarchical softmax or negative sampling performs better for this dataset.\n",
    "\n",
    "#### Model 4: CBOW and Hierarchical Softmax tuning\n",
    "\"polit\" - The model does not have high cosine similarity values (max 0.31) here but does pick words that somewhat make sense around \"political\", such as \"economy\", \"extremist\", and \"leadership\".\n",
    "\n",
    "\"thousand\" - The model still assigns a low max similarity value (0.35) but again chooses words that make sense in context, like \"million\", \"fifteen\", and \"twenty\". \n",
    "\n",
    "\"countri\" - We see the pattern continue here; the max cosine similarity value is a low 0.36, but the model captures words that make sense in context like \"world\", \"us\", and \"society\". \n",
    "\n",
    "Overall: This model does a decent job across all three word vectors despite assigning low cosine similarity values. It is interesting that it behaves so consistently across all three words, as models 1-3 behaved very differently across the three word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Part 2\n",
    "2. Read the required embeddings papers (word2vec, Bert, optionally Elmo) and compare the approaches in less than one page, preferably in a table format. You are free to decide what aspects to compare.  For example, you can include information such as: learning model details, summary of word context approaches, corpus size requirements, computational requirements, ease of installation/use of source code, date of publication and number of google scholar citations :), â€¦ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "- Learning Model Details:\n",
    "    - The training objective is to learn word vector representations that are good at predicting the nearby words.\n",
    "    - Simple extension from word basis to phrase basis (e.g. to pick out phrases like \"Boston Globe\", the newspaper).\n",
    "    - Extension of the skip-gram model, but can also be modified to use CBOW.\n",
    "    - Options for hierarchical softmax (an efficient approximation of full softmax) or negative sampling (NCE).\n",
    "    - Subsamples frequent words. This is similar to TF-IDF in that it decreases the impact of words that are used over and over (and therefore have less meaning in context). This technique significantly improves the accuracy of  the model on more rare words.\n",
    "- Word Context Approach:\n",
    "    - The model uses a skip-gram algorithm to train each word using the words around it. Phrases are formed based on unigram and bigram counts, so that the model can use some multi-grams without hugely increasing the size of vocabulary (as it would with n-gram use). \n",
    "    - However, when you are testing the model or finding similarity results, it does not matter where in the sentence or phrase a word occurs. In this sense it is somewhat context-independent.\n",
    "    - word2vec also does not attempt to classify parts of speech or otherwise place a word in its sentence.\n",
    "- Corpus Size Requirement:\n",
    "    - You need a large number of samples for word2vec. Generally a training corpus of 250,000 unique words is best.\n",
    "    - This paper uses a corpus with dimensionality of 300 and context size of 5. For the highest amount of accuracy, it used a corpus of 33 billion words (huge) and achieved an accuracy of 72%.\n",
    "    - A computationally efficient infrastructure (via skip-gram and hierarchical softmax) means that word2vec can train quickly on huge datasets.\n",
    "- Ease of Installation:\n",
    "    - word2vec is easiest to implement in C, but gensim produces a Python version as well which works well (above). The Python version works up to 70x faster if a C compiler is also installed, though for a relatively small corpus simple models still trained within a few seconds. If the C compiler is not installed, the gensim algorithm runs on Numpy implementations. Both gensim and numpy are simple to install.\n",
    "- Date of Publication:\n",
    "    - 7 Sep 2013\n",
    "- Number of Google Scholar Citations:\n",
    "    - 15320\n",
    "\n",
    "#### BERT\n",
    "- Learning Model Details:\n",
    "    - BERT improves on fine-tuning approaches (as opposed to feature-based approaches) by building a deep learning model that uses bidirectional training (breaking a previously-held assumption that training for NLP must flow from left to right). It does this by using Masked Language Models (MLMs). The LMs used must be \"masked\" since they are bidirectional; each pass, a random number of words are omitted in order to keep the word from being able to \"see\" (and thus predict) itself.\n",
    "    - BERT starts with a generalized pre-training step and then moves into context-specific fine-tuning steps that build on the pre-trained model with labelled data. \n",
    "    - It is versatile for many tasks because it represents \"sentences\" as continuous strings of words, rather than strictly as linguistic sentences. \n",
    "- Word Context Approach:\n",
    "    - Model focuses on Next Sentence Prediction (NSP) so that it can understand the relationship between sentences. This is beneficial for downstream tasks like question answering and natural language inference.\n",
    "    - BERT is therefore very contextual and very dependent on the Transform used. This means that it is fairly difficult to tune and implement, BUT that once tuned and fully trained it can answer questions and perform other more complex tasks than single-directional models like word2vec can.\n",
    "- Corpus Size Requirement:\n",
    "    - For each language, BERT must be pre-trained on a very large corpus (33 billion words for English). This must only be done once per language; afterwards, fine-tuning can be done by context and can be much faster and done with a much smaller dataset.\n",
    "    - In the paper, BERT fine-tuning does fairly well even with fairly small datasets. For example, the MPRC dataset ranges from an accuracy of about 79% to about 88%, depending on the size of the model. \n",
    "- Ease of Installation:\n",
    "    - BERT models are available for download and should run out of the box (though I have not downloaded, as I do not have the memory or RAM to run these models). \n",
    "    - The BERT documentation on Github (https://github.com/google-research/bert/blob/master/README.md) enumerates how computationally intensive its models are to train: it can 4 days on 4 to 16 TPUs to train BERT on a new language.\n",
    "    - Fine-tuning can be done quickly - in one hour on a single TPU for most datasets. \n",
    "- Date of Publication:\n",
    "    - Nov 2, 2018\n",
    "- Number of Google Scholar Citations:\n",
    "    - 1851 (many fewer than word2vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kernel",
   "language": "python",
   "name": "nlp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
